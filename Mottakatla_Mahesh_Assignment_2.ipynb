{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMR1318/Maheshreddy_INFO5731_Fall2024/blob/main/Mottakatla_Mahesh_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cUZ0gwzqpXBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ece6ef7-1bee-4c43-c485-ce433c5d5f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.25.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.26.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QHUTR7dVAmE1"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hzVrSebeBIN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d2d66a-1a6c-4904-b17e-cdfe386052ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install chromium-chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4b80Zz1WBrOM"
      },
      "outputs": [],
      "source": [
        "def web_driver():\n",
        "  options = webdriver.ChromeOptions()\n",
        "  options.add_argument (\"--verbose\")\n",
        "  options.add_argument('-no-sandbox')\n",
        "  options.add_argument('-headless')\n",
        "  options.add_argument('--disable-gpu')\n",
        "  options.add_argument (\"--window-size=1920, 1200\")\n",
        "  options.add_argument('--disable-dev-shm-usage')\n",
        "  driver = webdriver. Chrome (options=options)\n",
        "  return driver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jjwn7LY7B4GL"
      },
      "outputs": [],
      "source": [
        "driver = web_driver()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75A_UFm-CBMD",
        "outputId": "6b505c4f-5047-4e5b-8736-463bd8b79e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'chromium-chromedriver' instead of 'chromium-driver'\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n",
            "Scraping reviews for movie ID: tt15398776\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "Collected 500 reviews for movie ID: tt15398776\n",
            "Scraping reviews for movie ID: tt13238346\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "Collected 500 reviews for movie ID: tt13238346\n",
            "Collected 1000 reviews and saved to imdb_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "!apt-get install -y chromium-driver\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def web_driver():\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--verbose\")\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--disable-gpu')\n",
        "    options.add_argument(\"--window-size=1920,1200\")\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    return driver\n",
        "\n",
        "driver = web_driver()\n",
        "\n",
        "def get_imdb_reviews(movie_ids, reviews_per_movie=400, max_retries=2, retry_delay=5):\n",
        "    reviews_list = []\n",
        "    buttons_list = []  # List to hold button information\n",
        "\n",
        "    for movie_id in movie_ids:\n",
        "        url = f\"https://www.imdb.com/title/{movie_id}/reviews?ref_=tt_ql_3\"\n",
        "        print(f\"Scraping reviews for movie ID: {movie_id}\")\n",
        "        movie_reviews = []\n",
        "        retries = 0\n",
        "\n",
        "        while len(movie_reviews) < reviews_per_movie and retries < max_retries:\n",
        "            try:\n",
        "                driver.get(url)\n",
        "                time.sleep(5)  # Wait for the page to load\n",
        "\n",
        "                # Parse the page with BeautifulSoup\n",
        "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "                # print(soup.prettify())  # Print the complete HTML of the page for inspection\n",
        "\n",
        "                # Collect button information after parsing the page\n",
        "                buttons = soup.find('button', id='load-more-trigger')\n",
        "                print(buttons)\n",
        "\n",
        "                # Load more reviews\n",
        "                load_more_button = driver.find_element(By.ID, 'load-more-trigger')\n",
        "                load_more_button.click()\n",
        "                time.sleep(20)  # Wait for more reviews to load\n",
        "\n",
        "                # Collect reviews after clicking the load more button\n",
        "                reviews = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "                if not reviews:\n",
        "                    retries += 1\n",
        "                    time.sleep(retry_delay)\n",
        "                    continue  # Retry if no reviews are found\n",
        "\n",
        "                # Collect initial reviews\n",
        "                for review in reviews:\n",
        "                    if len(movie_reviews) >= reviews_per_movie:\n",
        "                        break\n",
        "                    movie_reviews.append(review.text.strip())\n",
        "\n",
        "                retries = 0  # Reset retries after a successful attempt\n",
        "                time.sleep(1)  # To avoid making too many requests quickly\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred for movie ID: {movie_id}: {e}\")\n",
        "                retries += 1\n",
        "                time.sleep(retry_delay)\n",
        "\n",
        "        reviews_list.extend(movie_reviews)\n",
        "        print(f\"Collected {len(movie_reviews)} reviews for movie ID: {movie_id}\")\n",
        "\n",
        "    # Close the driver after scraping\n",
        "    driver.quit()\n",
        "    return reviews_list\n",
        "\n",
        "def save_reviews_to_csv(reviews, filename):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Review\"])\n",
        "        for review in reviews:\n",
        "            writer.writerow([review])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # List of IMDb movie IDs to collect reviews from\n",
        "    movie_ids = [\n",
        "        \"tt15398776\",  # Oppenheimer\n",
        "        \"tt13238346\" # Past Lives\n",
        "        ]\n",
        "\n",
        "    reviews_per_movie = 500\n",
        "    reviews = get_imdb_reviews(movie_ids, reviews_per_movie=reviews_per_movie)\n",
        "\n",
        "    if reviews:\n",
        "        save_reviews_to_csv(reviews, 'imdb_reviews.csv')\n",
        "        print(f\"Collected {len(reviews)} reviews and saved to imdb_reviews.csv\")\n",
        "    else:\n",
        "        print(\"No reviews collected.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QX6bJjGWXY9",
        "outputId": "af3be00d-3741-4553-814b-e93ef95c7c63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned reviews saved to 'cleaned_imdb_reviews.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Ensure nltk resources are downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the reviews from the CSV file\n",
        "def load_reviews_from_csv(filename):\n",
        "    return pd.read_csv(filename)\n",
        "\n",
        "# Clean text function\n",
        "def clean_text(text):\n",
        "    # (1) Remove noise, such as special characters and punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # (2) Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # (3) Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text_tokens = nltk.word_tokenize(text)\n",
        "    text = ' '.join(word for word in text_tokens if word.lower() not in stop_words)\n",
        "\n",
        "    # (4) Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    # (5) Stemming\n",
        "    ps = PorterStemmer()\n",
        "    text = ' '.join(ps.stem(word) for word in text.split())\n",
        "\n",
        "    # (6) Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "# Main function to process the reviews\n",
        "def process_reviews(filename):\n",
        "    df = load_reviews_from_csv(filename)\n",
        "\n",
        "    # Apply the cleaning process\n",
        "    df['Cleaned_Review'] = df['Review'].apply(clean_text)\n",
        "\n",
        "    # Save the cleaned reviews to a new CSV file\n",
        "    df.to_csv('cleaned_imdb_reviews.csv', index=False, encoding='utf-8')\n",
        "    print(f\"Cleaned reviews saved to 'cleaned_imdb_reviews.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_reviews('imdb_reviews.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0oOSlsOS0cq",
        "outputId": "5f8c9bad-43b8-43af-eb8a-d5f69cc8f017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "!pip install nltk\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "um2QFKlkO1sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be583da6-57aa-460c-c9c3-4f7b3b15db80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gfW_SboZiOhj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0-pDSizNfuow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd64996-d303-4ac3-ecc3-d449000be325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total POS Counts:\n",
            "{'Noun': 68236, 'Verb': 19207, 'Adjective': 27741, 'Adverb': 7609}\n",
            "Constituency Parse Tree:\n",
            "(S\n",
            "  one/CD\n",
            "  (NP anticip/NN)\n",
            "  (NP film/NN)\n",
            "  (NP year/NN)\n",
            "  (NP mani/NN)\n",
            "  (NP peopl/NN)\n",
            "  (NP includ/NN)\n",
            "  (NP oppenheim/NN)\n",
            "  larg/VBZ\n",
            "  deliv/RB\n",
            "  (NP much/JJ great/JJ feel/NN)\n",
            "  (PP like/IN (NP love/NN))\n",
            "  two/CD\n",
            "  three/CD\n",
            "  (NP hour/NN)\n",
            "  (PP like/IN (NP hour/NN))\n",
            "  (NP fact/NN)\n",
            "  stop/VB\n",
            "  (NP ador/NN)\n",
            "  (NP entir/JJ thing/NN)\n",
            "  know/VBP\n",
            "  (NP christoph/NN)\n",
            "  (NP nolan/NN)\n",
            "  dunkirk/VBZ\n",
            "  (NP click/JJ second/JJ watch/NN)\n",
            "  (NP mayb/NN)\n",
            "  oppenheim/VBD\n",
            "  need/MD\n",
            "  one/CD\n",
            "  said/VBD\n",
            "  (NP dont/NN)\n",
            "  (NP feel/NN)\n",
            "  need/VBP\n",
            "  (NP rush/NN)\n",
            "  see/VB\n",
            "  soon/RB\n",
            "  long/RB\n",
            "  (NP exhaust/JJ filmbut/NN)\n",
            "  (NP mani/JJ way/NN)\n",
            "  (NP cant/JJ deni/NN)\n",
            "  except/IN\n",
            "  well/RB\n",
            "  made/VBN\n",
            "  one/CD\n",
            "  (NP look/NN)\n",
            "  (NP sound/NN)\n",
            "  amaz/IN\n",
            "  youd/JJ\n",
            "  expect/VBP\n",
            "  feel/VB\n",
            "  (PP though/IN (NP accur/JJ captur/JJ time/NN))\n",
            "  (NP period/NN)\n",
            "  set/VBN\n",
            "  (NP contain/NN)\n",
            "  (NP amaz/JJ sound/NN)\n",
            "  (NP design/NN)\n",
            "  one/CD\n",
            "  (NP year/NN)\n",
            "  best/RBS\n",
            "  (NP score/NN)\n",
            "  far/RB\n",
            "  (NP everi/JJ perform/NN)\n",
            "  (NP good/JJ great/JJ film/NN)\n",
            "  (NP belong/JJ cillian/JJ murphi/NN)\n",
            "  (NP feel/NN)\n",
            "  like/IN\n",
            "  he/PRP\n",
            "  lead/VBD\n",
            "  (NP actor/NN)\n",
            "  (NP beat/NN)\n",
            "  (NP stage/NN)\n",
            "  (NP talk/NN)\n",
            "  earli/VBZ\n",
            "  award/RB\n",
            "  (NP considerationth/JJ film/NN)\n",
            "  best/JJS\n",
            "  (NP focus/NN)\n",
            "  (NP psycholog/NN)\n",
            "  (NP thriller/NN)\n",
            "  (NP featur/NN)\n",
            "  (NP famou/JJ histor/NN)\n",
            "  figur/VBD\n",
            "  one/CD\n",
            "  (NP point/NN)\n",
            "  even/RB\n",
            "  turn/VBP\n",
            "  (NP psycholog/JJ horror/NN)\n",
            "  (NP film/NN)\n",
            "  there/RB\n",
            "  one/CD\n",
            "  (NP sequenc/NN)\n",
            "  (NP involv/NN)\n",
            "  (NP speech/NN)\n",
            "  (PP that/IN (NP particularli/NN))\n",
            "  (NP terrifi/NN)\n",
            "  also/RB\n",
            "  manag/VBZ\n",
            "  (NP suspens/NNS)\n",
            "  (NP moment/NN)\n",
            "  even/RB\n",
            "  (PP though/IN (NP stori/JJ commonli/NN))\n",
            "  known/VBN\n",
            "  (NP histori/JJ pointi/NN)\n",
            "  (NP realli/NN)\n",
            "  (NP feel/NN)\n",
            "  (PP length/IN (NP final/JJ hour/NN))\n",
            "  (PP though/IN (NP mayb/JJ wish/JJ final/JJ act/NN))\n",
            "  extend/VBP\n",
            "  (NP epilogu/NN)\n",
            "  rather/RB\n",
            "  whole/JJ\n",
            "  third/JJ\n",
            "  movi/FW\n",
            "  (NP current/JJ feel/NN)\n",
            "  (PP though/IN (NP wouldv/JJ love/NN))\n",
            "  oppenheim/RB\n",
            "  (NP hour/NN)\n",
            "  instead/RB\n",
            "  (NP noth/DT bad/JJ mean/NN)\n",
            "  (NP littl/NN)\n",
            "  (NP patienc/JJ test/NN)\n",
            "  (NP subject/NN)\n",
            "  (NP rememb/NN)\n",
            "  (NP feel/NN)\n",
            "  (PP like/IN (NP similarli/NN))\n",
            "  long/RB\n",
            "  (NP babylon/JJ total/NN)\n",
            "  (NP justifi/NN)\n",
            "  (NP runtim/NN)\n",
            "  (PP though/IN (NP other/JJ didnt/NN))\n",
            "  feel/VB\n",
            "  wayim/JJ\n",
            "  left/VBN\n",
            "  (NP feel/NN)\n",
            "  (PP like/IN (NP watch/NN))\n",
            "  (NP film/NN)\n",
            "  (NP wasnt/NN)\n",
            "  slam/VBP\n",
            "  (NP dunk/NN)\n",
            "  incred/VBN\n",
            "  (NP runtim/NN)\n",
            "  (NP wasnt/NN)\n",
            "  that/WDT\n",
            "  still/RB\n",
            "  worth/VBZ\n",
            "  (NP celebr/NNS)\n",
            "  make/VBP\n",
            "  oppenheim/JJ\n",
            "  worth/JJ\n",
            "  see/VBP\n",
            "  cinema/JJ\n",
            "  sure/JJ)\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        S                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
            "   _____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________|_________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________       \n",
            "  |       |        |       |       |        |       |          |            |          |      |       |        |       |       |       |        |        |       |       |       |       |        |         |       |      |       |       |       |      |       |         |        |        |         |       |       |       |        |       |       |        |        |        |         |          |        |        |        |         |           |         |       |       |        |        |         |         |        |         |        |          |          |        |        |        |        |         |       |       |       |         |          |                  |                      PP            |             PP            |       |              |                |          |                  |                 |       |       |       |                |                      |                  |             |       |                         PP                       |         |               |               |        |       |               |                        |                           |                   |       |        |       |        |                      |             |          |            |          |               |               |                   |               |        |          |         |              PP                     |           |          |                  PP                                |                |        |                 PP                             PP                               |                  |                        PP                   |              |               |                 |              |          |        |             PP                          |               |          |                  PP                    |             PP             |       |        |        |        |         |     \n",
            "  |       |        |       |       |        |       |          |            |          |      |       |        |       |       |       |        |        |       |       |       |       |        |         |       |      |       |       |       |      |       |         |        |        |         |       |       |       |        |       |       |        |        |        |         |          |        |        |        |         |           |         |       |       |        |        |         |         |        |         |        |          |          |        |        |        |        |         |       |       |       |         |          |                  |                 _____|_____        |        _____|_____        |       |              |                |          |                  |                 |       |       |       |                |                      |                  |             |       |          _______________|______                  |         |               |               |        |       |               |                        |                           |                   |       |        |       |        |                      |             |          |            |          |               |               |                   |               |        |          |         |         _____|_________             |           |          |          ________|______                           |                |        |         ________|______                ________|_______                         |                  |               _________|______              |              |               |                 |              |          |        |        _____|_______                    |               |          |          ________|______               |        _____|_____         |       |        |        |        |         |      \n",
            "  |       |        |       |       |        |       |          |            |          |      |       |        |       |       |       |        |        |       |       |       |       |        |         |       |      |       |       |       |      |       |         |        |        |         |       |       |       |        |       |       |        |        |        |         |          |        |        |        |         |           |         |       |       |        |        |         |         |        |         |        |          |          |        |        |        |        NP        NP      NP      NP      NP        NP         NP                 NP               |           NP      NP      |           NP      NP      NP             NP               NP         NP                 NP                NP      NP      NP      NP               NP                     NP                 NP            NP      NP        |                      NP                NP        NP              NP              NP       NP      NP              NP                       NP                          NP                  NP      NP       NP      NP       NP                     NP            NP         NP           NP         NP              NP              NP                  NP              NP       NP         NP        NP       |               NP           NP          NP         NP        |               NP                         NP               NP       NP       |               NP             |                NP                       NP                 NP             |                NP            NP             NP              NP                NP             NP         NP       NP      |             NP                  NP              NP         NP        |               NP             NP      |           NP       NP      NP       NP       NP       NP        NP    \n",
            "  |       |        |       |       |        |       |          |            |          |      |       |        |       |       |       |        |        |       |       |       |       |        |         |       |      |       |       |       |      |       |         |        |        |         |       |       |       |        |       |       |        |        |        |         |          |        |        |        |         |           |         |       |       |        |        |         |         |        |         |        |          |          |        |        |        |        |         |       |       |       |         |          |           _______|________        |           |       |       |           |       |       |        ______|_____           |          |         _________|________         |       |       |       |         _______|______           _____|____         _____|_____        |       |         |         _____________|________         |         |          _____|_____          |        |       |         ______|______           _______|________          _________|__________         |       |        |       |        |            __________|_____        |          |            |          |         ______|______         |           ________|______         |        |          |         |        |               |            |           |          |         |         ______|_______            _______|______          |        |        |         ______|_____         |         _______|_______________         |           _______|_____         |          ______|_____        |        ______|_______        |          _______|_____         |          |        |       |             |            _______|_____          |          |         |         ______|_____         |       |           |        |       |        |        |        |         |      \n",
            "one/CD larg/VBZ deliv/RB two/CD three/CD stop/VB know/VBP dunkirk/VBZ oppenheim/VBD need/MD one/CD said/VBD need/VBP see/VB soon/RB long/RB except/IN well/RB made/VBN one/CD amaz/IN youd/JJ expect/VBP feel/VB set/VBN one/CD best/RBS far/RB like/IN he/PRP lead/VBD earli/VBZ award/RB best/JJS figur/VBD one/CD even/RB turn/VBP there/RB one/CD also/RB manag/VBZ even/RB known/VBN extend/VBP rather/RB whole/JJ third/JJ movi/FW oppenheim/RB instead/RB long/RB feel/VB wayim/JJ left/VBN slam/VBP incred/VBN that/WDT still/RB worth/VBZ make/VBP oppenheim/JJ worth/JJ see/VBP cinema/JJ sure/JJ anticip/NN film/NN year/NN mani/NN peopl/NN includ/NN oppenheim/NN much/JJ great/JJ feel/NN like/IN     love/NN hour/NN like/IN     hour/NN fact/NN ador/NN entir/JJ     thing/NN christoph/NN nolan/NN click/JJ second/JJ watch/NN mayb/NN dont/NN feel/NN rush/NN exhaust/JJ     filmbut/NN mani/JJ     way/NN cant/JJ     deni/NN look/NN sound/NN though/IN accur/JJ     captur/JJ time/NN period/NN contain/NN amaz/JJ     sound/NN design/NN year/NN score/NN everi/JJ     perform/NN good/JJ great/JJ film/NN belong/JJ cillian/JJ murphi/NN feel/NN actor/NN beat/NN stage/NN talk/NN considerationth/     film/NN focus/NN psycholog/NN thriller/NN featur/NN famou/JJ     histor/NN point/NN psycholog/JJ     horror/NN film/NN sequenc/NN involv/NN speech/NN that/IN     particularli/NN terrifi/NN suspens/NNS moment/NN though/IN stori/JJ     commonli/NN histori/JJ     pointi/NN realli/NN feel/NN length/IN final/JJ     hour/NN though/IN mayb/JJ wish/JJ final/JJ act/NN epilogu/NN current/JJ     feel/NN though/IN wouldv/JJ     love/NN hour/NN noth/DT bad/JJ mean/NN littl/NN patienc/JJ     test/NN subject/NN rememb/NN feel/NN like/IN     similarli/NN babylon/JJ     total/NN justifi/NN runtim/NN though/IN other/JJ     didnt/NN feel/NN like/IN     watch/NN film/NN wasnt/NN dunk/NN runtim/NN wasnt/NN celebr/NNS\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 JJ                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
            "\n",
            "\n",
            "Dependency Parse Tree:\n",
            "one --> nummod --> anticip\n",
            "anticip --> compound --> film\n",
            "film --> compound --> year\n",
            "year --> compound --> larg\n",
            "mani --> compound --> peopl\n",
            "peopl --> compound --> includ\n",
            "includ --> compound --> larg\n",
            "oppenheim --> compound --> larg\n",
            "larg --> nsubj --> deliv\n",
            "deliv --> ROOT --> deliv\n",
            "much --> advmod --> great\n",
            "great --> amod --> feel\n",
            "feel --> dobj --> deliv\n",
            "like --> prep --> feel\n",
            "love --> pobj --> like\n",
            "two --> nummod --> hour\n",
            "three --> nummod --> hour\n",
            "hour --> npadvmod --> deliv\n",
            "like --> prep --> deliv\n",
            "hour --> pobj --> like\n",
            "fact --> npadvmod --> stop\n",
            "stop --> conj --> deliv\n",
            "ador --> npadvmod --> entir\n",
            "entir --> amod --> thing\n",
            "thing --> nsubj --> know\n",
            "know --> parataxis --> stop\n",
            "christoph --> compound --> dunkirk\n",
            "nolan --> compound --> dunkirk\n",
            "dunkirk --> nsubj --> click\n",
            "click --> conj --> deliv\n",
            "second --> compound --> watch\n",
            "watch --> dobj --> click\n",
            "mayb --> compound --> oppenheim\n",
            "oppenheim --> nsubj --> need\n",
            "need --> ccomp --> deliv\n",
            "one --> nsubj --> said\n",
            "said --> ccomp --> need\n",
            "do --> aux --> feel\n",
            "nt --> neg --> feel\n",
            "feel --> xcomp --> said\n",
            "need --> aux --> see\n",
            "rush --> nsubj --> see\n",
            "see --> xcomp --> feel\n",
            "soon --> advmod --> long\n",
            "long --> amod --> way\n",
            "exhaust --> compound --> filmbut\n",
            "filmbut --> nmod --> way\n",
            "mani --> compound --> way\n",
            "way --> nsubj --> made\n",
            "ca --> aux --> deni\n",
            "nt --> neg --> deni\n",
            "deni --> amod --> way\n",
            "except --> prep --> deni\n",
            "well --> advmod --> made\n",
            "made --> ccomp --> see\n",
            "one --> nummod --> look\n",
            "look --> nsubj --> sound\n",
            "sound --> ccomp --> made\n",
            "amaz --> prep --> sound\n",
            "you --> pobj --> amaz\n",
            "d --> nsubj --> expect\n",
            "expect --> parataxis --> deliv\n",
            "feel --> dobj --> expect\n",
            "though --> mark --> contain\n",
            "accur --> compound --> period\n",
            "captur --> compound --> period\n",
            "time --> compound --> period\n",
            "period --> nsubj --> contain\n",
            "set --> acl --> period\n",
            "contain --> advcl --> expect\n",
            "amaz --> advmod --> contain\n",
            "sound --> amod --> design\n",
            "design --> pobj --> amaz\n",
            "one --> nummod --> year\n",
            "year --> npadvmod --> best\n",
            "best --> advmod --> score\n",
            "score --> conj --> deliv\n",
            "far --> advmod --> everi\n",
            "everi --> amod --> film\n",
            "perform --> nmod --> film\n",
            "good --> amod --> film\n",
            "great --> amod --> film\n",
            "film --> compound --> belong\n",
            "belong --> compound --> murphi\n",
            "cillian --> compound --> murphi\n",
            "murphi --> advcl --> deliv\n",
            "feel --> conj --> deliv\n",
            "like --> mark --> lead\n",
            "he --> nsubj --> lead\n",
            "lead --> advcl --> feel\n",
            "actor --> compound --> earli\n",
            "beat --> compound --> earli\n",
            "stage --> compound --> earli\n",
            "talk --> compound --> earli\n",
            "earli --> compound --> award\n",
            "award --> compound --> considerationth\n",
            "considerationth --> compound --> film\n",
            "film --> nsubj --> focus\n",
            "best --> advmod --> focus\n",
            "focus --> compound --> thriller\n",
            "psycholog --> compound --> thriller\n",
            "thriller --> dobj --> lead\n",
            "featur --> npadvmod --> lead\n",
            "famou --> amod --> figur\n",
            "histor --> compound --> figur\n",
            "figur --> dobj --> lead\n",
            "one --> nummod --> point\n",
            "point --> npadvmod --> lead\n",
            "even --> advmod --> turn\n",
            "turn --> dep --> lead\n",
            "psycholog --> compound --> film\n",
            "horror --> compound --> film\n",
            "film --> dobj --> turn\n",
            "there --> advmod --> film\n",
            "one --> nummod --> sequenc\n",
            "sequenc --> nsubj --> involv\n",
            "involv --> conj --> deliv\n",
            "speech --> dobj --> involv\n",
            "that --> mark --> manag\n",
            "particularli --> compound --> terrifi\n",
            "terrifi --> nsubj --> manag\n",
            "also --> advmod --> manag\n",
            "manag --> relcl --> speech\n",
            "suspens --> compound --> moment\n",
            "moment --> dobj --> manag\n",
            "even --> advmod --> feel\n",
            "though --> mark --> feel\n",
            "stori --> compound --> commonli\n",
            "commonli --> nsubj --> feel\n",
            "known --> acl --> commonli\n",
            "histori --> compound --> realli\n",
            "pointi --> compound --> realli\n",
            "realli --> appos --> commonli\n",
            "feel --> advcl --> manag\n",
            "length --> nmod --> hour\n",
            "final --> amod --> hour\n",
            "hour --> npadvmod --> feel\n",
            "though --> mark --> wish\n",
            "mayb --> nsubj --> wish\n",
            "wish --> advcl --> feel\n",
            "final --> amod --> act\n",
            "act --> nsubj --> extend\n",
            "extend --> ccomp --> wish\n",
            "epilogu --> amod --> feel\n",
            "rather --> advmod --> whole\n",
            "whole --> amod --> movi\n",
            "third --> amod --> movi\n",
            "movi --> nmod --> feel\n",
            "current --> amod --> feel\n",
            "feel --> dobj --> extend\n",
            "though --> mark --> love\n",
            "wouldv --> nsubj --> love\n",
            "love --> advcl --> extend\n",
            "oppenheim --> compound --> hour\n",
            "hour --> dobj --> love\n",
            "instead --> advmod --> noth\n",
            "noth --> conj --> deliv\n",
            "bad --> amod --> mean\n",
            "mean --> prep --> deliv\n",
            "littl --> dep --> subject\n",
            "patienc --> compound --> test\n",
            "test --> npadvmod --> subject\n",
            "subject --> dep --> was\n",
            "rememb --> amod --> feel\n",
            "feel --> dobj --> subject\n",
            "like --> prep --> feel\n",
            "similarli --> nmod --> babylon\n",
            "long --> amod --> babylon\n",
            "babylon --> nmod --> runtim\n",
            "total --> amod --> runtim\n",
            "justifi --> compound --> runtim\n",
            "runtim --> pobj --> like\n",
            "though --> mark --> feel\n",
            "other --> nsubj --> feel\n",
            "did --> aux --> feel\n",
            "nt --> neg --> feel\n",
            "feel --> advcl --> subject\n",
            "wayim --> nsubj --> left\n",
            "left --> ccomp --> feel\n",
            "feel --> conj --> subject\n",
            "like --> mark --> was\n",
            "watch --> compound --> film\n",
            "film --> nsubj --> was\n",
            "was --> advcl --> feel\n",
            "nt --> neg --> was\n",
            "slam --> compound --> dunk\n",
            "dunk --> nmod --> runtim\n",
            "incred --> amod --> runtim\n",
            "runtim --> nsubj --> was\n",
            "was --> ROOT --> was\n",
            "nt --> neg --> was\n",
            "that --> advmod --> worth\n",
            "still --> advmod --> worth\n",
            "worth --> ccomp --> was\n",
            "celebr --> advmod --> worth\n",
            "make --> dep --> was\n",
            "oppenheim --> nsubj --> worth\n",
            "worth --> nsubj --> see\n",
            "see --> ccomp --> make\n",
            "cinema --> dobj --> see\n",
            "sure --> advcl --> was\n",
            "\n",
            "Total Named Entities Count:\n",
            "{'PERSON': 3247, 'ORG': 1086, 'GPE': 1192, 'PRODUCT': 56, 'DATE': 527}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from nltk import RegexpParser\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the cleaned reviews from the CSV file\n",
        "def load_cleaned_reviews(filename):\n",
        "    return pd.read_csv(filename)\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Count Nouns, Verbs, Adjectives, Adverbs\n",
        "    pos_counts = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "    for word, tag in pos_tags:\n",
        "        if tag.startswith('NN'):  # Noun\n",
        "            pos_counts['Noun'] += 1\n",
        "        elif tag.startswith('VB'):  # Verb\n",
        "            pos_counts['Verb'] += 1\n",
        "        elif tag.startswith('JJ'):  # Adjective\n",
        "            pos_counts['Adjective'] += 1\n",
        "        elif tag.startswith('RB'):  # Adverb\n",
        "            pos_counts['Adverb'] += 1\n",
        "\n",
        "    return pos_counts\n",
        "\n",
        "# (2) Constituency Parsing and Dependency Parsing\n",
        "def parse_sentence(sentence):\n",
        "    # Constituency Parsing\n",
        "    grammar = r\"\"\"\n",
        "        NP: {<DT>?<JJ>*<NN.*>}  # Noun Phrase\n",
        "        VP: {<VB.*><NP|PP|CLAUSE>+$}  # Verb Phrase\n",
        "        PP: {<IN><NP>}  # Prepositional Phrase\n",
        "        CLAUSE: {<NP><VP>}  # Clause\n",
        "    \"\"\"\n",
        "    cp = RegexpParser(grammar)\n",
        "    tree = cp.parse(nltk.pos_tag(word_tokenize(sentence)))\n",
        "    print(\"Constituency Parse Tree:\")\n",
        "    print(tree)\n",
        "    tree.pretty_print()\n",
        "\n",
        "    # Dependency Parsing\n",
        "    doc = nlp(sentence)\n",
        "    print(\"\\nDependency Parse Tree:\")\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")\n",
        "\n",
        "# (3) Named Entity Recognition\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Initialize an empty dictionary to count entities\n",
        "    entities_count = {}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        # Increment the count for each entity label dynamically\n",
        "        if ent.label_ in entities_count:\n",
        "            entities_count[ent.label_] += 1\n",
        "        else:\n",
        "            entities_count[ent.label_] = 1\n",
        "\n",
        "    return entities_count\n",
        "\n",
        "# Main function to analyze the cleaned reviews\n",
        "def analyze_reviews(filename):\n",
        "    df = load_cleaned_reviews(filename)\n",
        "\n",
        "    # Parts of Speech Tagging\n",
        "    total_pos_counts = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "\n",
        "    for review in df['Cleaned_Review']:\n",
        "        pos_counts = pos_tagging(review)\n",
        "        for key in total_pos_counts.keys():\n",
        "            total_pos_counts[key] += pos_counts[key]\n",
        "\n",
        "    print(\"Total POS Counts:\")\n",
        "    print(total_pos_counts)\n",
        "\n",
        "    # Sentence for Parsing\n",
        "    example_sentence = df['Cleaned_Review'].iloc[0]\n",
        "    parse_sentence(example_sentence)\n",
        "\n",
        "    # Named Entity Recognition\n",
        "    total_entities = {'PERSON': 0, 'ORG': 0, 'GPE': 0, 'PRODUCT': 0, 'DATE': 0}\n",
        "    for review in df['Cleaned_Review']:\n",
        "        entities_count = named_entity_recognition(review)\n",
        "        for key in total_entities.keys():\n",
        "            # Check if the key exists in entities_count before adding\n",
        "            if key in entities_count:\n",
        "                total_entities[key] += entities_count[key]\n",
        "\n",
        "    print(\"\\nTotal Named Entities Count:\")\n",
        "    print(total_entities)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyze_reviews('/content/cleaned_imdb_reviews.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXNn1lEVbMsv"
      },
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://myunt-my.sharepoint.com/:x:/r/personal/maheshreddymottakatla_my_unt_edu/Documents/cleaned_imdb_reviews.csv?d=wd2d91c2bfda4450b9251ad5ae25d416f&csf=1&web=1&e=8zCcLs"
      ],
      "metadata": {
        "id": "Am8rc180gx9i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "## Code Organization and Readability\n",
        "\n",
        "#I ensured that my code is clean and well-commented to make it easy for anyone reviewing it to understand the functionality. I also chose descriptive variable names to enhance readability.\n",
        "\n",
        "## Handling Data Collection\n",
        "\n",
        "#During the data collection process, I used both web scraping and API requests. I faced a few challenges with handling timeouts, but I implemented retries to ensure the data was collected reliably. I made sure to respect the site's terms and added a delay between requests to avoid overwhelming the server.\n",
        "\n",
        "## Data Cleaning Approach\n",
        "\n",
        "#I thoroughly cleaned the text data by removing irrelevant information like special characters, numbers, and stopwords. This step was essential to ensure accurate analysis. I also performed lemmatization to normalize the words, which helped in improving the syntactic analysis.\n",
        "\n",
        "## Syntactic Analysis\n",
        "\n",
        "#For syntactic analysis, I implemented POS tagging, constituency parsing, dependency parsing, and named entity recognition. One thing I found particularly interesting was how Named Entity Recognition (NER) helped in identifying key entities in the text, which could be useful for further analysis, such as extracting specific themes from the reviews.\n",
        "\n",
        "## Challenges Faced\n",
        "\n",
        "#One challenge I encountered was with handling large volumes of text data during cleaning and processing. To address this, I optimized my code by using vectorized operations where possible. Additionally, handling noisy data required some trial and error, especially when deciding on the best cleaning techniques for my dataset.\n",
        "\n",
        "## Reflection\n",
        "\n",
        "#Overall, I believe my approach to both data collection and analysis was effective, but theres room for improvement, especially in expanding the data sources to get more diverse results. In future projects, I would like to explore more advanced NLP models to gain deeper insights from the text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "outputs": [],
      "source": [
        "# Write your response below\n",
        "# Thoughts on the Assignment\n",
        "# Challenges:\n",
        "\n",
        "# Data Scraping Complexity: Initially, scraping data from IMDb using Selenium presented some challenges, particularly in handling dynamic content loading and ensuring that the necessary elements were available before interaction.\n",
        "# Text Cleaning and Processing: Implementing various text cleaning techniques was intricate, especially ensuring that each stepsuch as stemming and lemmatizationwas effectively applied without losing the essence of the reviews.\n",
        "# Syntax and Structure Analysis: Conducting parts of speech tagging, parsing, and named entity recognition required a solid understanding of NLP concepts and the libraries used, which was both challenging and rewarding.\n",
        "# Enjoyable Aspects:\n",
        "\n",
        "# Learning New Tools: Using libraries like NLTK and SpaCy for text processing and analysis was exciting. They offer powerful tools for NLP that broadened my understanding of the field.\n",
        "# Data Insights: Extracting meaningful insights from the cleaned data and conducting the analyses provided a sense of accomplishment. Seeing the results of the entity recognition and understanding the syntactic structure of sentences was particularly gratifying.\n",
        "# Practical Application: Applying theoretical knowledge in a real-world scenario, such as analyzing movie reviews, made the assignment engaging and relevant.\n",
        "# Time Management: The time provided to complete the assignment felt adequate. However, some tasks, such as troubleshooting the web scraping and ensuring the accuracy of the text analysis, required more time than initially anticipated. I appreciated having time to dive deeper into the aspects of NLP that I found intriguing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}