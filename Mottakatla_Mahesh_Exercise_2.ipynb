{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMR1318/Maheshreddy_INFO5731_Fall2024/blob/main/Mottakatla_Mahesh_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBKvD6O_TY6e"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "outputs": [],
      "source": [
        "# Data to be Collected:\n",
        "# We need to collect the following types of data:\n",
        "\n",
        "# 1. Weather Data:\n",
        "#    - Temperature (°C or °F)\n",
        "#    - Precipitation (mm or inches)\n",
        "#    - Humidity (%)\n",
        "#    - Wind Speed (in km/h or mph)\n",
        "#    - Major weather events (e.g., storms, droughts)\n",
        "#    - Source: Weather services or APIs (e.g., NOAA, Weather.com API, OpenWeatherMap API)\n",
        "\n",
        "# 2. Consumer Spending Data:\n",
        "#    - Daily/weekly/monthly retail sales\n",
        "#    - E-commerce sales (if applicable)\n",
        "#    - Type of expenditure (e.g., food, clothing, electronics)\n",
        "#    - Geographical regions (city/state/country)\n",
        "#    - Source: Government economic data, retail databases, financial institutions (e.g., Statista, World Bank, Kaggle)\n",
        "\n",
        "# 3. Demographic Data:\n",
        "#    - Population size\n",
        "#    - Income levels\n",
        "#    - Employment rates\n",
        "#    - Age distribution\n",
        "#    - Source: Census data, demographic surveys, public databases\n",
        "\n",
        "# Amount of Data:\n",
        "# - Collect data for at least 2-3 years to account for seasonal adjustments and major weather events.\n",
        "# - The dataset should include:\n",
        "#    - Weather Data: Daily records for all regions.\n",
        "#    - Spending Data: Monthly or weekly trends in retail categories.\n",
        "#    - Demographic Data: Latest available demographic data for each region.\n",
        "# - For large studies, the dataset may consist of millions of rows.\n",
        "\n",
        "# Steps for Collecting and Saving Data:\n",
        "\n",
        "# 1. Identify Data Sources:\n",
        "#    - Choose weather APIs (e.g., OpenWeatherMap) and government sources for spending data.\n",
        "#    - Identify APIs or databases for demographic data.\n",
        "\n",
        "# 2. Access the Data:\n",
        "#    - Use APIs (e.g., OpenWeatherMap) to retrieve daily weather data for the relevant geographic areas.\n",
        "#    - Gather monthly/weekly consumer spending data from government or retail databases.\n",
        "#    - Retrieve demographic data from census databases or other national sources.\n",
        "\n",
        "# 3. Data Collection Process:\n",
        "#    - Weather Data:\n",
        "#      - Use Python or another programming language to call weather APIs.\n",
        "#      - Pass the required date and regions as parameters in the API call.\n",
        "#      - Store the data in JSON or CSV format for further analysis.\n",
        "#    - Consumer Spending Data:\n",
        "#      - Collect retail data from public sources or use scraping techniques with financial data provider APIs.\n",
        "#      - Pool the data by region and time (weekly/monthly intervals).\n",
        "#    - Demographic Data:\n",
        "#      - Use Python libraries like Pandas and BeautifulSoup to scrape or download demographic data for the selected regions.\n",
        "\n",
        "# 4. Data Cleaning and Preprocessing:\n",
        "#    - Use Pandas to clean missing or inconsistent data.\n",
        "#    - Standardize data (e.g., convert all temperatures to °C).\n",
        "#    - Classify spending data into major retail categories.\n",
        "\n",
        "# 5. Save the Data:\n",
        "#    - Store weather, spending, and demographic data in a structured format:\n",
        "#      - Use CSV files for easier analysis.\n",
        "#      - For large datasets, consider using a relational database (e.g., SQLite, MySQL) for faster querying.\n",
        "#      - For scalability, consider using cloud storage (e.g., AWS S3).\n",
        "\n",
        "# 6. Data Validation:\n",
        "#    - Ensure data integrity by:\n",
        "#      - Comparing totals across datasets.\n",
        "#      - Validating API response codes.\n",
        "#      - Cross-referencing data from multiple sources (e.g., comparing data from two weather APIs).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RqrlwdTfvl"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "4XvRknixTh1g",
        "outputId": "7f626245-5444-4abe-8532-2fe08c8b8793"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"city\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Delhi\",\n          \"Paris\",\n          \"Tokyo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temperature\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.868816929518076,\n        \"min\": 13.46,\n        \"max\": 37.96,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          17.14,\n          17.31,\n          37.96\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"humidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 39,\n        \"max\": 96,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          45,\n          39,\n          59\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wind_speed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.7617030417317863,\n        \"min\": 2.57,\n        \"max\": 15.43,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          6.26,\n          6.69,\n          8.49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weather_event\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"clear sky\",\n          \"few clouds\",\n          \"haze\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"food_spending\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 71.7059775413114,\n        \"min\": 50.03,\n        \"max\": 299.49,\n        \"num_unique_values\": 978,\n        \"samples\": [\n          63.53,\n          156.75,\n          238.17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clothing_spending\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 52.8322177236504,\n        \"min\": 20.25,\n        \"max\": 199.96,\n        \"num_unique_values\": 975,\n        \"samples\": [\n          83.99,\n          34.18,\n          114.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"electronics_spending\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 263.7275959822107,\n        \"min\": 100.49,\n        \"max\": 998.7,\n        \"num_unique_values\": 997,\n        \"samples\": [\n          220.46,\n          642.25,\n          889.39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_spending\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 721.2250552889662,\n        \"min\": 502.22,\n        \"max\": 2998.26,\n        \"num_unique_values\": 996,\n        \"samples\": [\n          593.32,\n          614.03,\n          810.82\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"population_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5545956,\n        \"min\": 515416,\n        \"max\": 19985849,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          1304032,\n          11060785,\n          13206480\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"income_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23377.775231936335,\n        \"min\": 20005.58,\n        \"max\": 99998.54,\n        \"num_unique_values\": 999,\n        \"samples\": [\n          33206.51,\n          72748.11,\n          81642.27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"employment_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.025726069628236,\n        \"min\": 60.01,\n        \"max\": 94.94,\n        \"num_unique_values\": 864,\n        \"samples\": [\n          89.63,\n          81.46,\n          81.96\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age_distribution\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"50+\",\n          \"35-50\",\n          \"18-35\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-5924351d-5a13-4acb-b595-f353839ac31c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>city</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>wind_speed</th>\n",
              "      <th>weather_event</th>\n",
              "      <th>food_spending</th>\n",
              "      <th>clothing_spending</th>\n",
              "      <th>electronics_spending</th>\n",
              "      <th>total_spending</th>\n",
              "      <th>population_size</th>\n",
              "      <th>income_level</th>\n",
              "      <th>employment_rate</th>\n",
              "      <th>age_distribution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dubai</td>\n",
              "      <td>37.96</td>\n",
              "      <td>39</td>\n",
              "      <td>6.69</td>\n",
              "      <td>clear sky</td>\n",
              "      <td>73.39</td>\n",
              "      <td>38.82</td>\n",
              "      <td>887.39</td>\n",
              "      <td>2296.73</td>\n",
              "      <td>17749626</td>\n",
              "      <td>21087.68</td>\n",
              "      <td>70.24</td>\n",
              "      <td>50+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Paris</td>\n",
              "      <td>15.53</td>\n",
              "      <td>60</td>\n",
              "      <td>4.12</td>\n",
              "      <td>few clouds</td>\n",
              "      <td>50.90</td>\n",
              "      <td>39.47</td>\n",
              "      <td>757.95</td>\n",
              "      <td>844.66</td>\n",
              "      <td>16417172</td>\n",
              "      <td>40528.80</td>\n",
              "      <td>81.70</td>\n",
              "      <td>50+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sydney</td>\n",
              "      <td>13.53</td>\n",
              "      <td>75</td>\n",
              "      <td>15.43</td>\n",
              "      <td>scattered clouds</td>\n",
              "      <td>294.70</td>\n",
              "      <td>31.88</td>\n",
              "      <td>295.92</td>\n",
              "      <td>1429.60</td>\n",
              "      <td>13073281</td>\n",
              "      <td>35818.20</td>\n",
              "      <td>72.71</td>\n",
              "      <td>35-50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Toronto</td>\n",
              "      <td>16.05</td>\n",
              "      <td>96</td>\n",
              "      <td>4.12</td>\n",
              "      <td>mist</td>\n",
              "      <td>137.91</td>\n",
              "      <td>110.47</td>\n",
              "      <td>444.94</td>\n",
              "      <td>1872.78</td>\n",
              "      <td>14049884</td>\n",
              "      <td>89368.66</td>\n",
              "      <td>90.48</td>\n",
              "      <td>18-35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>New York</td>\n",
              "      <td>19.01</td>\n",
              "      <td>88</td>\n",
              "      <td>2.57</td>\n",
              "      <td>clear sky</td>\n",
              "      <td>270.90</td>\n",
              "      <td>168.83</td>\n",
              "      <td>265.30</td>\n",
              "      <td>2685.16</td>\n",
              "      <td>5357384</td>\n",
              "      <td>40211.57</td>\n",
              "      <td>66.37</td>\n",
              "      <td>35-50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5924351d-5a13-4acb-b595-f353839ac31c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5924351d-5a13-4acb-b595-f353839ac31c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5924351d-5a13-4acb-b595-f353839ac31c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3a00c44b-ccac-4b13-a883-ececcd9503cc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3a00c44b-ccac-4b13-a883-ececcd9503cc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3a00c44b-ccac-4b13-a883-ececcd9503cc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "       city  temperature  humidity  wind_speed     weather_event  \\\n",
              "0     Dubai        37.96        39        6.69         clear sky   \n",
              "1     Paris        15.53        60        4.12        few clouds   \n",
              "2    Sydney        13.53        75       15.43  scattered clouds   \n",
              "3   Toronto        16.05        96        4.12              mist   \n",
              "4  New York        19.01        88        2.57         clear sky   \n",
              "\n",
              "   food_spending  clothing_spending  electronics_spending  total_spending  \\\n",
              "0          73.39              38.82                887.39         2296.73   \n",
              "1          50.90              39.47                757.95          844.66   \n",
              "2         294.70              31.88                295.92         1429.60   \n",
              "3         137.91             110.47                444.94         1872.78   \n",
              "4         270.90             168.83                265.30         2685.16   \n",
              "\n",
              "   population_size  income_level  employment_rate age_distribution  \n",
              "0         17749626      21087.68            70.24              50+  \n",
              "1         16417172      40528.80            81.70              50+  \n",
              "2         13073281      35818.20            72.71            35-50  \n",
              "3         14049884      89368.66            90.48            18-35  \n",
              "4          5357384      40211.57            66.37            35-50  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Define API key and endpoint for OpenWeatherMap\n",
        "API_KEY = \"cdcee05e3d7b54158bd244757081bf5d\"\n",
        "BASE_URL = \"http://api.openweathermap.org/data/2.5/weather\"\n",
        "\n",
        "# List of cities to collect data from (for simplicity, we use a few cities)\n",
        "cities = [\"London\", \"New York\", \"Berlin\", \"Tokyo\", \"Sydney\", \"Toronto\", \"Delhi\", \"Paris\", \"Dubai\", \"Moscow\"]\n",
        "\n",
        "# Function to fetch weather data from OpenWeatherMap API\n",
        "def fetch_weather_data(city):\n",
        "    params = {\n",
        "        'q': city,\n",
        "        'appid': API_KEY,\n",
        "        'units': 'metric'  # Using Celsius for temperature\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return {\n",
        "            'city': city,\n",
        "            'temperature': data['main']['temp'],\n",
        "            'humidity': data['main']['humidity'],\n",
        "            'wind_speed': data['wind']['speed'],\n",
        "            'weather_event': data['weather'][0]['description']\n",
        "        }\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to generate random consumer spending data\n",
        "def generate_spending_data():\n",
        "    return {\n",
        "        'food_spending': round(random.uniform(50, 300), 2),\n",
        "        'clothing_spending': round(random.uniform(20, 200), 2),\n",
        "        'electronics_spending': round(random.uniform(100, 1000), 2),\n",
        "        'total_spending': round(random.uniform(500, 3000), 2)\n",
        "    }\n",
        "\n",
        "# Function to generate random demographic data\n",
        "def generate_demographic_data():\n",
        "    return {\n",
        "        'population_size': random.randint(500000, 20000000),\n",
        "        'income_level': round(random.uniform(20000, 100000), 2),\n",
        "        'employment_rate': round(random.uniform(60, 95), 2),\n",
        "        'age_distribution': random.choice(['18-35', '35-50', '50+'])\n",
        "    }\n",
        "\n",
        "# Initialize an empty list to store the collected data\n",
        "data = []\n",
        "\n",
        "# Collect 1000 samples\n",
        "for i in range(1000):\n",
        "    city = random.choice(cities)\n",
        "\n",
        "    # Fetch weather data for the city\n",
        "    weather_data = fetch_weather_data(city)\n",
        "\n",
        "    if weather_data:\n",
        "        # Generate consumer spending and demographic data\n",
        "        spending_data = generate_spending_data()\n",
        "        demographic_data = generate_demographic_data()\n",
        "\n",
        "        # Combine all data into a single dictionary\n",
        "        combined_data = {**weather_data, **spending_data, **demographic_data}\n",
        "        data.append(combined_data)\n",
        "\n",
        "    # To avoid hitting API rate limits, sleep for a short time between requests\n",
        "    time.sleep(0.2)\n",
        "\n",
        "# Convert the collected data into a Pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "df.to_csv('weather_consumer_spending_data.csv', index=False)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "YaGLbSHHB8Ej",
        "outputId": "6018222d-6eb5-46dc-fa02-02ab890bd99d"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-bd088a94f6aa>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Convert the results to a DataFrame and save to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-bd088a94f6aa>\u001b[0m in \u001b[0;36mcollect_articles\u001b[0;34m(keyword, start_year, end_year, num_articles)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed URL: {url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Delay to avoid being blocked by Google Scholar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m# Move to the next page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Function to collect data from Google Scholar\n",
        "def collect_articles(keyword, start_year, end_year, num_articles=5):\n",
        "    i = 0\n",
        "    results = []\n",
        "\n",
        "    # Create a query string with the provided keyword\n",
        "    query = f\"+{keyword.replace(' ', '+')}\"\n",
        "\n",
        "    while len(results) < num_articles:\n",
        "        url = f'https://scholar.google.com/scholar?start={i}&q={query}&hl=en&as_sdt=0,5&as_ylo={start_year}&as_yhi={end_year}'\n",
        "        content = requests.get(url).text\n",
        "        page = BeautifulSoup(content, 'lxml')\n",
        "\n",
        "        try:\n",
        "            for entry in page.find_all(\"div\", attrs={\"class\": \"gs_ri\"}):\n",
        "                title = entry.find('h3', attrs={'class': 'gs_rt'})\n",
        "                author = entry.find('div', attrs={'class': 'gs_a'})\n",
        "                abst = entry.find('div', attrs={'class': 'gs_rs'})\n",
        "                cite = entry.find('div', attrs={'class': 'gs_fl'})\n",
        "\n",
        "                # Collect the required details\n",
        "                results.append({\n",
        "                    \"title\": title.text if title else \"No title\",\n",
        "                    \"url\": entry.a['href'] if entry.a else \"No URL\",\n",
        "                    \"authors\": author.text if author else \"No authors\",\n",
        "                    \"abstract\": abst.text if abst else \"No abstract\",\n",
        "                    \"citation\": cite.text.replace('Cited by', '').strip() if cite else \"No citation\",\n",
        "                    \"year\": end_year\n",
        "                })\n",
        "\n",
        "                # Break the loop if the required number of articles are collected\n",
        "                if len(results) >= num_articles:\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An exception occurred: {e}\")\n",
        "            print(f\"Failed URL: {url}\")\n",
        "\n",
        "        time.sleep(5) # Delay to avoid being blocked by Google Scholar\n",
        "        i += 10 # Move to the next page\n",
        "\n",
        "    return results\n",
        "\n",
        "# Collect 1000 articles with the keyword \"XYZ\" from 2014 to 2024\n",
        "keyword = \"XYZ\"\n",
        "start_year = 2014\n",
        "end_year = 2024\n",
        "num_articles = 5\n",
        "\n",
        "articles = collect_articles(keyword, start_year, end_year, num_articles)\n",
        "\n",
        "# Convert the results to a DataFrame and save to CSV\n",
        "df = pd.DataFrame(articles)\n",
        "df.to_csv('google_data.csv', mode='a', header=False, index=False)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# 1. Introduction to Octoparse\n",
        "# Octoparse is a no-code web scraping tool that allows users to extract data from websites easily.\n",
        "# It provides a visual interface to configure scraping tasks and export data in various formats such as CSV or Excel.\n",
        "\n",
        "# 2. Steps for Web Scraping\n",
        "\n",
        "# A. Setup Octoparse\n",
        "\n",
        "# Download and Install Octoparse:\n",
        "# 1. Go to the Octoparse website and download the software.\n",
        "# 2. Install it on your computer and sign up or log in.\n",
        "\n",
        "# Create a New Task:\n",
        "# 1. Open Octoparse and click on + New Task.\n",
        "# 2. Enter the URL of the Reddit subreddit you want to scrape, e.g., https://www.reddit.com/r/technology/.\n",
        "# 3. Click Start to load the page in Octoparse's built-in browser.\n",
        "\n",
        "# B. Configure Scraping Rules\n",
        "\n",
        "# Point-and-Click Selection:\n",
        "# 1. Click on the post titles on the Reddit page to select them. Octoparse will highlight similar elements.\n",
        "# 2. Choose Select all to capture all post titles.\n",
        "# 3. Click on the author names, post dates, and content to add them as fields. Name these fields as \"Post Title,\" \"Username,\" \"Post Date,\" and \"Content.\"\n",
        "\n",
        "# Loop Pagination (if needed):\n",
        "# 1. If you want to scrape multiple pages, click on the Next Page button and choose Loop Click.\n",
        "\n",
        "# Preview and Adjust:\n",
        "# 1. Use the Preview tab to check the data extracted so far. Make sure all fields are correctly mapped.\n",
        "\n",
        "# C. Run the Task\n",
        "\n",
        "# Run Locally or in the Cloud:\n",
        "# 1. Click Run. You can choose to run it locally or in Octoparse's cloud (if you have a cloud plan).\n",
        "\n",
        "# Monitor Progress:\n",
        "# 1. Check the progress and ensure that the data is being extracted correctly.\n",
        "\n",
        "# D. Export the Data\n",
        "\n",
        "# Export Options:\n",
        "# 1. After the scraping task is complete, go to the Export section.\n",
        "# 2. Choose CSV or Excel as the export format.\n",
        "# 3. Click Export and save the file to your computer.\n",
        "\n",
        "\n",
        "# 3. Document Preparation\n",
        "# Create a Word or PDF document that includes:\n",
        "# - An introduction to Octoparse.\n",
        "# - Detailed steps of the scraping process.\n",
        "# - Screenshots of the Octoparse setup, including the point-and-click interface, data preview, and export process.\n",
        "# - A link to the exported CSV or Excel file.\n",
        "\n",
        "# 5. Upload and Share\n",
        "\n",
        "# Upload Document:\n",
        "# 1. Upload the document to a shared storage service like UNT OneDrive, Google Drive, or Dropbox.\n",
        "# 2. Ensure the document is publicly accessible.\n",
        "\n",
        "# Generate Shareable Link:\n",
        "# 1. Obtain a shareable link for the document.\n",
        "\n",
        "# Link: https://drive.google.com/file/d/15uR17-et9cl6zVJp0Z5CqaJ8IhddAtSM/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOhks1dXWEe"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqmHVEwaWhbV"
      },
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "akAVJn9YBTQT",
        "outputId": "94c40cab-b3f5-4c72-b072-32a80e21743d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Reflective Feedback on Web Scraping and Data Collection\\n\\n# Learning Experience\\n\\n# Blogs, articles, and other web contents were explored while solving the web scraping tasks\\n# to get efficiency in data scraping methods. Here are some key takeaways:\\n\\n# 1. Understanding Web Scraping Fundamentals:\\n# - HTML and CSS Selectors: \\n#   # The most important for attaining proficiency in identification of both HTML elements and CSS selectors proved to be essential.\\n#   # This knowledge was useful in determining the particular data to be scraped out of web pages.\\n# - Data Extraction Techniques:\\n#   # Something as simple as the use of point-and-click interface such as Octoparse or general understanding of how to set up an XPath or CSS selector for scraping was basic.\\n#   # Practical methods which are vital in data collection are these.\\n\\n# 2. Automation and Efficiency:\\n# - Task Automation:\\n#   # To fit large numbers of records, data were preprocessed using the automation tools, such as Octoparse, which proved the concept of automated data collection.\\n# - Error Handling:\\n#   # Dealing with certain obstacles for example CAPTCHA or data formatting problems also instilled humility and heart for error and data checks.\\n\\n# 3. Data Export and Integration:\\n# - Export Formats:\\n#   # Coming to know that exporting data in formats such as CSV and Excel was useful for further data manipulation and integration of the data into other tools was helpful.\\n# - Data Cleaning:\\n#   # The process of exporting data then cleaning and preprocessing it was very useful in showing how data preparation is critical in obtaining accurate and reliable results.\\n\\n# Challenges Encountered\\n\\n# 1. Website Structure and Scraping Difficulties:\\n# - Dynamic Content:\\n#   # Some of the websites allowed dynamic content loading through JavaScript, which made the extraction process a bit challenging.\\n#   # For this, I used tools that could work with AJAX or dealt with browser automation methods.\\n# - CAPTCHAs and Anti-Scraping Mechanisms:\\n#   # Websites with CAPTCHAs or any form of anti-scraping technologies proved to be very difficult.\\n#   # For these, I either employed headless browsers or proxy servers in order to unblock access.\\n\\n# 2. Tool-Specific Challenges:\\n# - Octoparse:\\n#   # Octoparse is easy to use, though I faced problems while working with pagination settings as well as sites with intricate structures.\\n#   # Some of the operations were easy to accomplish because of the visual interface, but others needed extra actions while configuring the parameters of complex workflows.\\n\\n# 3. Non-Coding Option Experience:\\n# - Ease of Use:\\n#   # Option №2 was a no-code solution that used the Octoparse tool, which anyone could utilize easily without prior programming knowledge.\\n# - Limitations:\\n#   # Although it is relatively straightforward to use, there exist problems with processing large data or for actively changing content: supplementary methods or tools were needed.\\n\\n# Relevance to Your Field of Study\\n\\n# 1. Enhanced Data Collection:\\n# - Comprehensive Research:\\n#   # Collecting data from various sources on the internet helps to expand the dataset of collected data.\\n#   # It is especially useful in disciplines such as market research, sociology, and technology studies.\\n\\n# 2. Data-Driven Insights:\\n# - Informed Decision-Making:\\n#   # Real-time data and historical data make it possible to perform well-analyzed decisions and trend analysis.\\n#   # It is important when creating tactics, analyzing customer trends, or evaluating market competitiveness.\\n\\n# 3. Skill Development:\\n# - Technical Proficiency:\\n#   # The knowledge of web scraping tools and methods improves technical competencies.\\n#   # Technical competence is critical in the current world since data-driven positions are widespread.\\n#   # It is applied to activities such as academic writing, research, business intelligence, and data analytics.\\n\\n# In general, it can be stated that the work with web scraping as well as the data collection tools has been quite enlightening and tends to be quite demanding at the same time.\\n# It has thus offered a way of how the process of extracting intelligence from data can be done and why it is necessary to be flexible when dealing with the data.\\n# These are very crucial skills as they ensure that the data gathered from the internet is used in meaningful research by different disciplines.\\n\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "# Reflective Feedback on Web Scraping and Data Collection\n",
        "\n",
        "# Learning Experience\n",
        "\n",
        "# Blogs, articles, and other web contents were explored while solving the web scraping tasks\n",
        "# to get efficiency in data scraping methods. Here are some key takeaways:\n",
        "\n",
        "# 1. Understanding Web Scraping Fundamentals:\n",
        "# - HTML and CSS Selectors:\n",
        "#   # The most important for attaining proficiency in identification of both HTML elements and CSS selectors proved to be essential.\n",
        "#   # This knowledge was useful in determining the particular data to be scraped out of web pages.\n",
        "# - Data Extraction Techniques:\n",
        "#   # Something as simple as the use of point-and-click interface such as Octoparse or general understanding of how to set up an XPath or CSS selector for scraping was basic.\n",
        "#   # Practical methods which are vital in data collection are these.\n",
        "\n",
        "# 2. Automation and Efficiency:\n",
        "# - Task Automation:\n",
        "#   # To fit large numbers of records, data were preprocessed using the automation tools, such as Octoparse, which proved the concept of automated data collection.\n",
        "# - Error Handling:\n",
        "#   # Dealing with certain obstacles for example CAPTCHA or data formatting problems also instilled humility and heart for error and data checks.\n",
        "\n",
        "# 3. Data Export and Integration:\n",
        "# - Export Formats:\n",
        "#   # Coming to know that exporting data in formats such as CSV and Excel was useful for further data manipulation and integration of the data into other tools was helpful.\n",
        "# - Data Cleaning:\n",
        "#   # The process of exporting data then cleaning and preprocessing it was very useful in showing how data preparation is critical in obtaining accurate and reliable results.\n",
        "\n",
        "# Challenges Encountered\n",
        "\n",
        "# 1. Website Structure and Scraping Difficulties:\n",
        "# - Dynamic Content:\n",
        "#   # Some of the websites allowed dynamic content loading through JavaScript, which made the extraction process a bit challenging.\n",
        "#   # For this, I used tools that could work with AJAX or dealt with browser automation methods.\n",
        "# - CAPTCHAs and Anti-Scraping Mechanisms:\n",
        "#   # Websites with CAPTCHAs or any form of anti-scraping technologies proved to be very difficult.\n",
        "#   # For these, I either employed headless browsers or proxy servers in order to unblock access.\n",
        "\n",
        "# 2. Tool-Specific Challenges:\n",
        "# - Octoparse:\n",
        "#   # Octoparse is easy to use, though I faced problems while working with pagination settings as well as sites with intricate structures.\n",
        "#   # Some of the operations were easy to accomplish because of the visual interface, but others needed extra actions while configuring the parameters of complex workflows.\n",
        "\n",
        "# 3. Non-Coding Option Experience:\n",
        "# - Ease of Use:\n",
        "#   # Option №2 was a no-code solution that used the Octoparse tool, which anyone could utilize easily without prior programming knowledge.\n",
        "# - Limitations:\n",
        "#   # Although it is relatively straightforward to use, there exist problems with processing large data or for actively changing content: supplementary methods or tools were needed.\n",
        "\n",
        "# Relevance to Your Field of Study\n",
        "\n",
        "# 1. Enhanced Data Collection:\n",
        "# - Comprehensive Research:\n",
        "#   # Collecting data from various sources on the internet helps to expand the dataset of collected data.\n",
        "#   # It is especially useful in disciplines such as market research, sociology, and technology studies.\n",
        "\n",
        "# 2. Data-Driven Insights:\n",
        "# - Informed Decision-Making:\n",
        "#   # Real-time data and historical data make it possible to perform well-analyzed decisions and trend analysis.\n",
        "#   # It is important when creating tactics, analyzing customer trends, or evaluating market competitiveness.\n",
        "\n",
        "# 3. Skill Development:\n",
        "# - Technical Proficiency:\n",
        "#   # The knowledge of web scraping tools and methods improves technical competencies.\n",
        "#   # Technical competence is critical in the current world since data-driven positions are widespread.\n",
        "#   # It is applied to activities such as academic writing, research, business intelligence, and data analytics.\n",
        "\n",
        "# In general, it can be stated that the work with web scraping as well as the data collection tools has been quite enlightening and tends to be quite demanding at the same time.\n",
        "# It has thus offered a way of how the process of extracting intelligence from data can be done and why it is necessary to be flexible when dealing with the data.\n",
        "# These are very crucial skills as they ensure that the data gathered from the internet is used in meaningful research by different disciplines.\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}